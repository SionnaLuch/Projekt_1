import psycopg2
from transformers import AutoTokenizer, AutoModel
import torch
import torch.nn.functional as F
import os

# ZaÅ‚aduj model do embeddingÃ³w (MiniLM - szybki i dobry)
tokenizer = AutoTokenizer.from_pretrained("sentence-transformers/all-MiniLM-L6-v2")
model = AutoModel.from_pretrained("sentence-transformers/all-MiniLM-L6-v2")

# Funkcja do przeliczenia tekstu na wektor
def embed(text):
    encoded = tokenizer(text, return_tensors="pt", truncation=True, padding=True)
    with torch.no_grad():
        model_output = model(**encoded)
    return F.normalize(model_output.last_hidden_state[:, 0, :], p=2, dim=1)

# PoÅ‚Ä…cz siÄ™ z bazÄ…
conn = psycopg2.connect(
    dbname="projekt_1",
    user="admin",
    password="haslo123",
    host="localhost",
    port=5432
)
cur = conn.cursor()
cur.execute("SELECT nazwa, opis FROM zdjecia WHERE opis IS NOT NULL;")
rows = cur.fetchall()
print(f"ğŸ“„ ZaÅ‚adowano {len(rows)} opisÃ³w z bazy.")

# Wczytaj wszystkie opisy + zdjÄ™cia
zdjecia = []
embedings = []

for nazwa, opis in rows:
    zdjecia.append((nazwa, opis))
    vec = embed(opis)
    embedings.append(vec)

# Pobierz zapytanie uÅ¼ytkownika
query = input("Wpisz, co chcesz znaleÅºÄ‡ (np. kobieta z niebieskimi oczami): ")
query_vec = embed(query)

# Oblicz podobieÅ„stwo kosinusowe
similarities = [F.cosine_similarity(query_vec, e)[0].item() for e in embedings]

# Posortuj i pokaÅ¼ 5 najlepszych
wyniki = sorted(zip(similarities, zdjecia), reverse=True)[:5]

print("\nğŸ” Najbardziej pasujÄ…ce zdjÄ™cia:\n")
for sim, (nazwa, opis) in wyniki:
    print(f"ğŸ“¸ {nazwa} ({sim:.2f})\nğŸ“ {opis}\n")